{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to the root directory of the project\n",
    "import os\n",
    "if os.getcwd().split(\"/\")[-1] == \"examples\":\n",
    "    os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to run this notebook on Google Colab, we first have to install `ScanDy` and download the required dataset from Google drive. The following code cell will prepare all of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the ScanDy framework via pip\n",
    "!pip install scandy\n",
    "\n",
    "# download the VidCom_example dataset from google drive using gdown\n",
    "!pip install gdown\n",
    "# dataset is stored at https://drive.google.com/file/d/1oT9OJ2tRsvdJGFFLSKDCaY3BJev4Irzf/view?usp=sharing\n",
    "file_id = '1oT9OJ2tRsvdJGFFLSKDCaY3BJev4Irzf'\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "output = 'vidcom_example.zip'\n",
    "!gdown $url -O $output\n",
    "!unzip $output\n",
    "\n",
    "# make visualizations directory for the output (if not cloned from github)\n",
    "!mkdir visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scandy.models.LocationModel import LocationModel\n",
    "from scandy.models.ObjectModel import ObjectModel\n",
    "from scandy.utils.dataclass import Dataset\n",
    "import scandy.utils.functions as uf\n",
    "\n",
    "from neurolib.utils.parameterSpace import ParameterSpace\n",
    "from neurolib.optimize.evolution import Evolution\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of simulated scanpaths\n",
    "\n",
    "The visualizations created in this notebook are stored in the `ScanDy/visualizations/` folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "ScanDy assumes video information to be already precomputed. The paths for the precomputed maps can be provided when initializing the dataset. If no information is given, it assumes the following file structure:\n",
    "\n",
    "```\n",
    "DATAPATH/\n",
    "├── videos/                 # Folder containing the videos (only for visualization)\n",
    "├── featuremaps/            # Folder containing the precomputed saliency maps\n",
    "    ├── molin/              #   The name of the subfolder is not required,   \n",
    "    ├── TASEDnet/           #   but has to match params['featureset'].\n",
    "    └── /.../ \n",
    "├── polished_segmentation/  # Folder with object segmentation masks\n",
    "├── optical_flow/           # Folder with optical flow maps (e.g. PWC net)\n",
    "└── gt_fov_maps_333/        # Optional, if NSS scores are to be computed \n",
    "                            # (smoothed human gaze positions)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = {\n",
    "    \"PATH\": \"VidCom_example/\",  # previously downloaded & extracted dataset  \n",
    "    'FPS' : 30,\n",
    "    'PX_TO_DVA' : 0.06,\n",
    "    'FRAMES_ALL_VIDS' : 300,\n",
    "    'gt_foveation_df' : 'VidCom_GT_fov_df.csv',\n",
    "    \"outputpath\" : os.getcwd()+\"/visualizations/\"  # path for saving the visualizations\n",
    "}\n",
    "VidCom = Dataset(datadict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a model and specify parameters\n",
    "\n",
    "We initialize an instance from the object-based model family. We here use low-level saliency maps from Molin et al., which corresponds to the `O_ll` model in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_ll = ObjectModel(VidCom)\n",
    "# low level features\n",
    "O_ll.params[\"featuretype\"] = \"molin\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the free model parameters with the average parameters from the evolutionary optimization described in the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_ll.params[\"ddm_thres\"] = 1.873\n",
    "O_ll.params[\"ddm_sig\"] = 0.241\n",
    "O_ll.params[\"att_dva\"] = 13.72\n",
    "O_ll.params[\"ior_decay\"] = 198.9\n",
    "O_ll.params[\"ior_inobj\"] = 0.76"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and evaluate for a single video\n",
    "\n",
    "Given the model and the dataset, we can now run the scanpath simulation. First we only run it a single time and choose a random seed for reproducibility.\n",
    "\n",
    "Running this should only take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_ll.run('field03', seeds = [10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the events (i.e. saccadic decisions and resulting foveations) of the simulated scanpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_ll.evaluate_all_to_df()\n",
    "O_ll.result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now qualitatively assess how reasonable this predicted scanpath is by plotting it on top of the \"observed\" video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the different modules for a single scanpath\n",
    "\n",
    "The models have a method that visualizes what's going on in the different modules (I-V) of the model while the scanpath is simulated. The creation of the gif will take multiple minutes and a lot of RAM (typically more than a laptop has or colab provides, so you might want to skip this cell and look at the visualizations provided on [GitHub](https://github.com/rederoth/ScanDy/tree/main/visualizations))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_ll.write_sgl_output_gif('field03_Oll_mean_sglrun', slowgif=True, dpi=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is then saved as a gif (specified outputpath in `Dataset`) and can be displayed in the notebook with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(VidCom.outputpath + 'field03_Oll_mean_sglrun_slow.gif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gif shows the simulated gaze position (green cross) on top of visualizations of the different modules of the model. The bottom left panel shows the object masks on top of the original video (shown with 10 fps instead of 30 fps).\n",
    "\n",
    "(I) Precomputed low-level saliency map with anisotropic center bias. Low values are shown in dark, high values in bright colors. \n",
    "\n",
    "(II) Gaze dependent visual sensitivity map, Gaussian with a uniform spread across currently foveated objects. Black means not sensitive (0), white means fully sensitive (1).\n",
    "\n",
    "(III) Visualization of the inhibition of return value of each object (attribute of the `ObjectFile` instance). White means no inhibition (0), black means fully inhibited (1).\n",
    "\n",
    "(IV) Visualization of the decision variable of each object (attribute of the `ObjectFile` instance). The saturation of the object mask represents the amount of accumulated evidence (white corresponds to 0, dark blue/red/green/orange to the decision threshold $\\theta$). When an object (including the background in blue) is fully saturated, it means that the decision variable has reached the threshold and a saccade is made to (or within) this object. After a saccade, all decision variables are reset to zero.\n",
    "\n",
    "(V) The red circle indicates the next gaze position. The pixel values indicate for each object how likely each position within each object is as a saccade target (calculated from the features (I) and sensitivity (II), $F\\times S$). If no saccade is made, the gaze point moves with the currently foveated object, resulting in either fixation or smooth pursuit behavior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate and visualize multiple scanpaths\n",
    "Due to the stochasticity of the scanpath generation, a single run is not sufficient to assess the quality of the model predictions. We therefore run the simulation multiple times and plot the scanpaths on top of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_ll.run('field03', seeds = [s for s in range(1, 13)], overwrite_old=True)\n",
    "O_ll.evaluate_all_to_df(overwrite_old=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_ll.video_output_gif('field03', 'field03_Oll_mean', slowgif=False, dpi=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this visualization every color corresponds to a different simulated scanpath (i.e., a different random seed). The video is shown with 30 fps, as in the eye tracking data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(VidCom.outputpath + 'field03_Oll_mean.gif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location-based model\n",
    "\n",
    "Lastly, we repeat the above steps for the location-based model with low-level features, `L_ll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_ll = LocationModel(VidCom)\n",
    "\n",
    "L_ll.params[\"featuretype\"] = \"molin\"\n",
    "L_ll.params[\"ddm_thres\"] = 0.355\n",
    "L_ll.params[\"ddm_sig\"] = 0.013\n",
    "L_ll.params[\"att_dva\"] = 12.77\n",
    "L_ll.params[\"ior_decay\"] = 226.5\n",
    "L_ll.params[\"ior_dva\"] = 6.82\n",
    "\n",
    "L_ll.run('field03', seeds = [10])\n",
    "L_ll.evaluate_all_to_df()\n",
    "L_ll.result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_ll.write_sgl_output_gif('field03_Lll_mean_sglrun', slowgif=True, dpi=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogously to the object-based model above, this gif shows the simulated gaze position (green cross) on top of visualizations of the different modules of the model. The bottom left panel shows the original video (shown with 10 fps instead of 30 fps).\n",
    "\n",
    "(I) Precomputed low-level saliency map with anisotropic center bias. Low values are shown in dark, high values in bright colors. \n",
    "\n",
    "(II) Gaze dependent Gaussian visual sensitivity map. Black means not sensitive (0), white means fully sensitive (1).\n",
    "\n",
    "(III) Inhibition of return map (value calculated for every pixel). White means no inhibition (0), black means fully inhibited (1)\n",
    "\n",
    "(IV) Visualization of the decision variable of each pixel-location. The saturation of a pixel represents the amount of accumulated evidence (white corresponds to 0, dark red to the decision threshold $\\theta$).\n",
    "\n",
    "(V) The red circle indicates the next gaze position. The pixel values indicate the optical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(VidCom.outputpath + 'field03_Lll_mean_sglrun_slow.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_ll.run('field03', seeds = [s for s in range(1, 13)], overwrite_old=True)\n",
    "L_ll.evaluate_all_to_df(overwrite_old=True)\n",
    "L_ll.video_output_gif('field03', 'field03_Lll_mean', slowgif=False, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(VidCom.outputpath + 'field03_Lll_mean.gif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By just comparing the resulting scanpaths on this one video (\"field03\" is part of the test set), we can see that the location-based model is not able to appropriately capture the way humans would explore the scene. The object-based model, on the other hand, leads to scanpaths which are hard to distinguish from human scanpaths."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a7dae82b130ec9d80cfefc4262e34c3661939d310e111163fc3ded324ad374c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

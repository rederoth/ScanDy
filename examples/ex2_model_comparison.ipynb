{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to the root directory of the project\n",
    "import os\n",
    "if os.getcwd().split(\"/\")[-1] == \"examples\":\n",
    "    os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to run this notebook on Google Colab, we first have to install `ScanDy` and download the required dataset from Google drive. The following code cell will prepare all of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the ScanDy framework via pip\n",
    "!pip install scandy\n",
    "\n",
    "# download the VidCom_example dataset from google drive using gdown\n",
    "!pip install gdown\n",
    "# dataset is stored at https://drive.google.com/file/d/1oT9OJ2tRsvdJGFFLSKDCaY3BJev4Irzf/view?usp=sharing\n",
    "file_id = '1oT9OJ2tRsvdJGFFLSKDCaY3BJev4Irzf'\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "output = 'vidcom_example.zip'\n",
    "!gdown $url -O $output\n",
    "!unzip $output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scandy.models.LocationModel import LocationModel\n",
    "from scandy.models.ObjectModel import ObjectModel\n",
    "from scandy.utils.dataclass import Dataset\n",
    "import scandy.utils.functions as uf\n",
    "\n",
    "from neurolib.utils.parameterSpace import ParameterSpace\n",
    "from neurolib.optimize.evolution import Evolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model optimization and quantitative comparison\n",
    "\n",
    "In this example, we show how we can find suitable parameters for a model with evolutionary optimization, such that models can be compared quantitatively. \n",
    "\n",
    "## Dataset and model initialization\n",
    "The parameter optimization should be done on the training set, but for this example we demonstrate how to do this on a single video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidlist = sorted(['dance01', 'dance02', 'field03', 'foutain02', 'garden04', 'garden06', 'garden07', 'garden09', 'park01', 'park06', 'park09', 'road02', 'road04', 'road05', 'robarm01', 'room01', 'room02', 'room03', 'tommy02', 'uscdog01', 'walkway01', 'walkway02', 'walkway03'])\n",
    "random.seed(12345)\n",
    "# this split is used for the results in the manuscript\n",
    "trainlist = sorted(random.sample(vidlist, 10))\n",
    "testlist = sorted([vidname for vidname in vidlist if vidname not in trainlist])\n",
    "print(\"trainlist = \", trainlist, \"\\ntestlist =\", testlist)\n",
    "# but here we only use one example video...\n",
    "\n",
    "datadict = {\n",
    "    \"PATH\": \"VidCom_example/\",  # previously downloaded & extracted dataset  \n",
    "    'FPS' : 30,\n",
    "    'PX_TO_DVA' : 0.06,\n",
    "    'FRAMES_ALL_VIDS' : 300,\n",
    "    'gt_foveation_df' : 'VidCom_GT_fov_df.csv',\n",
    "    'trainset' : [\"field03\"]  # trainlist,\n",
    "    # 'testset' : testlist,\n",
    "}\n",
    "VidCom = Dataset(datadict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only run it for a single model here, which is specified by the model family (MODEL) and the saliency information used (FEATURESET). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"obj\" # \"loc\" or \"obj\"\n",
    "# we can vary the saliency map, \"molin\" corresponds to low-level features\n",
    "FEATURESET = \"molin\" # \"molin\", \"TASEDnet\" (high-level), or \"None\" (center bias only)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the fitness function\n",
    "\n",
    "With this information, we can specify the model we want to investigate. The parameters are then selected for each run based on the evolutionary optimization strategy. \n",
    "\n",
    "But before we specify the parameter space, we define an optimization function, which we will pass to the `Evolution` class from `neurolib` (powered by the `deap` and `pypet` libraries).\n",
    "\n",
    "We specify a fitness function $\\mathcal{F}$ to evaluate how well the simulated scanpaths match the human data. In this example we evaluate each parameter configuration by compating the summary statistics of the saccade amplitude and foveation duration of the simulated scanpaths with the human ground truth using the Kolmogorov-Smirnov test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth summary statistics from the training set, this is used to evaluate the fitness\n",
    "gt_amp_dva = VidCom.train_foveation_df[\"sac_amp_dva\"].dropna().values\n",
    "gt_dur_ms = VidCom.train_foveation_df[\"duration_ms\"].dropna().values\n",
    "\n",
    "fig, axs = plt.subplots(1,2,dpi=100, figsize=(8,3), sharey=True)\n",
    "sns.histplot(data=np.log10(gt_dur_ms), kde=False, ax=axs[0], bins=40)\n",
    "axs[0].set_xticks([1,2,3,4]); axs[0].set_xticklabels([10,100,1000,10000])\n",
    "axs[0].set_xlabel('Foveation duration [ms]'); axs[0].set_ylabel('Count')\n",
    "sns.histplot(data=gt_amp_dva, kde=False, ax=axs[1], bins=60, label=\"training set\")\n",
    "axs[1].set_xlabel('Saccade amplitude [dva]')\n",
    "sns.despine(); plt.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is implemented in the following optimization function, where the model is initialized in every run with the parameters selected by the evolutionary optimization strategy and then run for all videos in the training set (in our example just a single video).\n",
    "The resulting scanpath(s) are then compared to the human data according to the fitness function $\\mathcal{F}$ and the results are stored in the HDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_me(traj):\n",
    "    ind = evolution.getIndividualFromTraj(traj)\n",
    "    # create model of the given model family\n",
    "    if MODEL == \"obj\":\n",
    "        model = ObjectModel(VidCom)\n",
    "    elif MODEL == \"loc\":\n",
    "        model = LocationModel(VidCom)\n",
    "    else:\n",
    "        raise Exception(f\"The only implemented models here are obj and loc!\")\n",
    "    model.params[\"centerbias\"] = \"anisotropic_default\"\n",
    "    model.params[\"featuretype\"] = FEATURESET\n",
    "    # free model parameters, varied in evolution\n",
    "    model.params[\"ddm_thres\"] = ind.ddm_thres\n",
    "    model.params[\"ddm_sig\"] = ind.ddm_sig\n",
    "    model.params[\"att_dva\"] = ind.att_dva\n",
    "    model.params[\"ior_decay\"] = ind.ior_decay\n",
    "    # IOR parameters depend on the model...\n",
    "    if MODEL == \"obj\":\n",
    "        model.params[\"ior_inobj\"] = ind.ior_inobj\n",
    "    else:\n",
    "        model.params[\"ior_dva\"] = ind.ior_dva\n",
    "\n",
    "    # usually run model on all videos of the training set for multiple random seeds\n",
    "    # model.run(\"train\", seeds=[s for s in range(1, 13)])\n",
    "    # ...but for demonstration purposes, we only run the model on one video once:\n",
    "    model.run(\"field03\", seeds=[s for s in [1]])\n",
    "\n",
    "    model.evaluate_all_to_df()  # creates model.result_df\n",
    "    sim_dur_ms = model.result_df[\"duration_ms\"].dropna().values\n",
    "    sim_amp_dva = model.result_df[\"sac_amp_dva\"].dropna().values\n",
    "\n",
    "    # evaluate fitness\n",
    "    ks_amp, _ = stats.ks_2samp(gt_amp_dva, sim_amp_dva)\n",
    "    ks_dur, _ = stats.ks_2samp(gt_dur_ms, sim_dur_ms)\n",
    "    fitness_tuple = (ks_dur, ks_amp)\n",
    "\n",
    "    # we can store more information in the HDF file by returning a dictionary\n",
    "    res_dict = model.get_fovcat_ratio()\n",
    "    \n",
    "    return fitness_tuple, res_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify parameters and run optimization\n",
    "\n",
    "We can now specify the parameter space for the optimization. We use the `ParameterSpace` class from `neurolib` to give a range for all free model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_pars = ParameterSpace(\n",
    "    [\"ddm_thres\", \"ddm_sig\", \"att_dva\", \"ior_decay\", \"ior_inobj\"],\n",
    "    [[1.0, 3.0], [0.05, 0.25], [5, 20], [30, 300], [0.4, 1.0]],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything set up to actually run the evolutionary optimization. As specified in `optimize_me`, we only run it for one video and one random seed. \n",
    "\n",
    "The processes are parallelized as much as your system allows for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution = Evolution(\n",
    "    optimize_me,\n",
    "    obj_pars,\n",
    "    weightList=[-1.0, -1.0], #weights foveation duration and saccade amplitude equally\n",
    "    filename=\"ex2_obj_sglvid.hdf\",\n",
    "    POP_INIT_SIZE=6,  # better: 64\n",
    "    POP_SIZE=6,  # better: 32\n",
    "    NGEN=5,  # better: 50\n",
    ")\n",
    "# verbose means it creates multiple plots for each generation\n",
    "evolution.run(verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, this evolution would be run on all videos of the trainingset, with multiple seeds, higher population sizes and for more generations. Hence, this typically runs best on a high-performance computing cluster.\n",
    "\n",
    "The results can be saved in a DILL file with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution.saveEvolution(\"saved_evolution_ex2_obj_sglvid.dill\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further analysis / functional evaluation\n",
    "\n",
    "To actually evaluate how well the simulated scanpaths correspond to human exploration behavior, we recommend to look into the functional scanpath comparison. The basis of this are the ratios of time that are spent in each foveation category, which we saved in this example in the result dictionary. \n",
    "\n",
    "The outputs from the evolution in the hdf file can be accessed in in a convenient way as pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evol = evolution.dfEvolution(outputs=True)\n",
    "# show the tree best parameter combinations with their fitness score and the stored results \n",
    "df_evol.sort_values(by=[\"score\"], ascending=False).iloc[0:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more in depth analysis of the foveation categories, please refer to the `manuscript_results.ipynb` notebook, where the full analysis is shown in detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a7dae82b130ec9d80cfefc4262e34c3661939d310e111163fc3ded324ad374c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
